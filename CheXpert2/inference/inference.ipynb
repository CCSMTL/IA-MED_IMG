{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import from pypi\n",
    "import tqdm\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#local import\n",
    "from CheXpert2.models.CNN import CNN\n",
    "from CheXpert2.Metrics import Metrics\n",
    "from CheXpert2.dataloaders.CXRLoader import CXRLoader\n",
    "from CheXpert2 import names,hierarchy\n",
    "\n",
    "for key in hierarchy.keys():\n",
    "    if key not in names :\n",
    "        names.insert(0,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    " def load_my_state_dict(self, state_dict):\n",
    "\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state:\n",
    "                 continue\n",
    "\n",
    "            own_state[name].copy_(param)\n",
    "\n",
    "def load_model() :\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        warnings.warn(\"No gpu is available for the computation\")\n",
    "    models = [\n",
    "        CNN(\"convnext_base_384_in22ft1k\", channels=3, num_classes=len(names), pretrained=False,hierarchical=True),\n",
    "        #    CNN(\"convnext_base\", img_size=384, channels=1, num_classes=14, pretrained=False, pretraining=False),\n",
    "        #    CNN(\"densenet201\", img_size=384, channels=1, num_classes=14, pretrained=False, pretraining=False),\n",
    "        #    CNN(\"densenet201\", img_size=384, channels=1, num_classes=14, pretrained=False, pretraining=False),\n",
    "    ]\n",
    "    # model =  torch.nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    # api = wandb.Api()\n",
    "    # run = api.run(f\"ccsmtl2/Chestxray/{args.run_id}\")\n",
    "    # run.file(\"models_weights/convnext_base/DistributedDataParallel.pt\").download(replace=True)\n",
    "    weights = [\n",
    "        \"/mnt/f/IA-med_img/data/model_weights/atomic-wildflower.pt\",\n",
    "        #    \"/data/home/jonathan/IA-MED_IMG/models_weights/convnext_base_2.pt\",\n",
    "        #    \"/data/home/jonathan/IA-MED_IMG/models_weights/densenet201.pt\",\n",
    "        #    \"/data/home/jonathan/IA-MED_IMG/models_weights/densenet201_2.pt\",\n",
    "    ]\n",
    "\n",
    "    for model, weight in zip(models, weights):\n",
    "        state_dict = torch.load(weight, map_location=torch.device(device))\n",
    "\n",
    "\n",
    "        #model.load_state_dict(state_dict)\n",
    "        load_my_state_dict(model,state_dict)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_loop(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model: model to evaluate\n",
    "    :param loader: dataset loader\n",
    "    :param criterion: criterion to evaluate the loss\n",
    "    :param device: device to do the computation on\n",
    "    :return: val_loss for the N epoch, tensor of concatenated labels and predictions\n",
    "    \"\"\"\n",
    "    running_loss = 0\n",
    "    results = [torch.tensor([]), torch.tensor([])]\n",
    "\n",
    "    for inputs, labels,idx in tqdm.tqdm(loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        inputs, labels = (\n",
    "            inputs.to(device, non_blocking=True),\n",
    "            labels.to(device, non_blocking=True),\n",
    "        )\n",
    "        #inputs,labels = loader.dataset.advanced_transform((inputs, labels))\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        running_loss += loss.detach()\n",
    "\n",
    "        if inputs.shape != labels.shape:  # prevent storing images if training unets\n",
    "            results[1] = torch.cat(\n",
    "                (results[1], outputs.detach().cpu()), dim=0\n",
    "            )\n",
    "            results[0] = torch.cat((results[0], labels.cpu()), dim=0)\n",
    "\n",
    "        del (\n",
    "            inputs,\n",
    "            labels,\n",
    "            outputs,\n",
    "            loss,\n",
    "        )  # garbage management sometimes fails with cuda\n",
    "\n",
    "\n",
    "    return running_loss, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Server not available ; switching offline\n",
      "/mnt/f/IA-med_img/CheXpert2/dataloaders/CXRLoader.py:184: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  data = data.groupby(\"Exam ID\").mean().round(0)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    warnings.warn(\"No gpu is available for the computation\")\n",
    "\n",
    "# ----- parsing arguments -------------------------------------\n",
    "#start = torch.cuda.Event(enable_timing=True)\n",
    "#end = torch.cuda.Event(enable_timing=True)\n",
    "# ------loading test set --------------------------------------\n",
    "#img_dir = os.environ[\"img_dir\"]\n",
    "img_dir = \"/mnt/f/IA-med_img\"\n",
    "os.environ[\"DEBUG\"]=\"True\"\n",
    "test_dataset = CXRLoader(\"Valid\",img_dir, img_size=384,channels=3,datasets=[\"ChexPert\"],prob=[0,]*5)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# ----------------loading model -------------------------------\n",
    "\n",
    "model=load_model()\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:30<00:00,  3.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.979350805282593\n",
      "(200, 10)\n",
      "{'auc': {'Enlarged Cardiomediastinum': 0.6470175438596492, 'Cardiomegaly': 0.6791044776119404, 'Pleural Effusion': 0.7545955882352942, 'Pneumothorax': 0.7173539518900344, 'Lung Opacity': 0.8075738916256159, 'Atelectasis': 0.80064, 'Pneumonia': 0.7884114583333333, 'Consolidation': 0.7816220238095237, 'Edema': 0.7649186256781193, 'No Finding': 0.6945181255526084, 'mean': 0.7435755686596119}, 'f1': {'Enlarged Cardiomediastinum': 0.6885245901639345, 'Cardiomegaly': 0.5205479452054794, 'Pleural Effusion': 0.5311203319502075, 'Pneumothorax': 0.059113300492610835, 'Lung Opacity': 0.7750865051903114, 'Atelectasis': 0.5967741935483871, 'Pneumonia': 0.08839779005524862, 'Consolidation': 0.3121951219512195, 'Edema': 0.37914691943127954, 'No Finding': 0.9304812834224598, 'mean': 0.4881387981411138}, 'recall': {'Enlarged Cardiomediastinum': 1.0, 'Cardiomegaly': 0.8636363636363636, 'Pleural Effusion': 1.0, 'Pneumothorax': 1.0, 'Lung Opacity': 0.9655172413793104, 'Atelectasis': 0.9866666666666667, 'Pneumonia': 1.0, 'Consolidation': 1.0, 'Edema': 0.9523809523809523, 'No Finding': 1.0}, 'precision': {'Enlarged Cardiomediastinum': 0.525, 'Cardiomegaly': 0.37254901960784315, 'Pleural Effusion': 0.3615819209039548, 'Pneumothorax': 0.030456852791878174, 'Lung Opacity': 0.6473988439306358, 'Atelectasis': 0.4277456647398844, 'Pneumonia': 0.046242774566473986, 'Consolidation': 0.18497109826589594, 'Edema': 0.23668639053254437, 'No Finding': 0.87}, 'accuracy': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#start.record()\n",
    "import time\n",
    "start = time.time()\n",
    "running_loss, results = infer_loop(model=model, loader=test_loader, criterion=criterion, device=device)\n",
    "#end.record()\n",
    "end=time.time()\n",
    "#torch.cuda.synchronize()\n",
    "#print(\"time : \", start.elapsed_time(end))\n",
    "print(end-start)\n",
    "#plt.imshow(np.sum(heatmaps[0][0].detach().cpu().numpy(), axis=0))\n",
    "#plt.savefig(\"heatmaps.png\")\n",
    "\n",
    "metric = Metrics(num_classes=len(names), names=names, threshold=np.zeros((len(names))) + 0.5)\n",
    "metrics = metric.metrics()\n",
    "metrics_results = {}\n",
    "for key in metrics:\n",
    "    pred = results[1].numpy()\n",
    "    true = results[0].numpy().round(0)\n",
    "    metric_result = metrics[key](true, pred)\n",
    "    metrics_results[key] = metric_result\n",
    "\n",
    "print(metrics_results)\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 14\u001B[0m\n\u001B[1;32m     12\u001B[0m path0\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc:/Users/joeda/PycharmProjects/IA-MED_IMG/data/1808916906062113_view0001.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     13\u001B[0m path1\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mc:/Users/joeda/PycharmProjects/IA-MED_IMG/data/1808916906062113_view0002.jpg\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m---> 14\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(path0)\n\u001B[1;32m     15\u001B[0m image0 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(cv\u001B[38;5;241m.\u001B[39mresize(cv\u001B[38;5;241m.\u001B[39mimread(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimg_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mpath0\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, cv\u001B[38;5;241m.\u001B[39mIMREAD_GRAYSCALE),(img_size,img_size)))\n\u001B[1;32m     16\u001B[0m image1 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray(cv\u001B[38;5;241m.\u001B[39mresize(cv\u001B[38;5;241m.\u001B[39mimread(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mimg_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mpath1\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m, cv\u001B[38;5;241m.\u001B[39mIMREAD_GRAYSCALE),(img_size,img_size)))\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FOR A SPECIFIC IMAGE\n",
    "\"\"\"\n",
    "#inputs,labels=test_dataset[i]\n",
    "#Manually loading images\n",
    "import cv2 as cv\n",
    "from pytorch_grad_cam import FullGrad\n",
    "from PIL import Image\n",
    "cam=FullGrad(model,target_layers=[])\n",
    "img_size=384\n",
    "img_dir=\"\"\n",
    "path0=\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/data/1808916906062113_view0001.jpg\"\n",
    "path1=\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/data/1808916906062113_view0002.jpg\"\n",
    "assert os.path.exists(path0)\n",
    "image0 = np.array(cv.resize(cv.imread(f\"{img_dir}{path0}\", cv.IMREAD_GRAYSCALE),(img_size,img_size)))\n",
    "image1 = np.array(cv.resize(cv.imread(f\"{img_dir}{path1}\", cv.IMREAD_GRAYSCALE),(img_size,img_size)))\n",
    "inputs = np.concatenate([image0[None,:,:],image1[None,:,:]],axis=0)\n",
    "inputs=inputs[None,:,:,:]\n",
    "# inputs, labels = (\n",
    "#             inputs.to(device, non_blocking=True),\n",
    "#             labels.to(device, non_blocking=True),\n",
    "#         )\n",
    "\n",
    "\n",
    "inputs = torch.from_numpy(inputs).to(device,non_blocking=True)\n",
    "# forward + backward + optimize\n",
    "outputs = model(inputs)\n",
    "img0=inputs[:,0:1,:,:]\n",
    "heatmap0 = cam(img0.float()).squeeze() * -255+255\n",
    "\n",
    "img1=inputs[:,1:2,:,:]\n",
    "heatmap1 = cam(img1.float()).squeeze() * -255+255\n",
    "\n",
    "\n",
    "heatmap0 = np.array(cv.applyColorMap(cv.cvtColor(heatmap0[:, :, None].astype(np.uint8), cv.COLOR_RGB2BGR),\n",
    "                                          cv.COLORMAP_JET))\n",
    "\n",
    "heatmap1 = np.array(cv.applyColorMap(cv.cvtColor(heatmap1[:, :, None].astype(np.uint8), cv.COLOR_RGB2BGR),\n",
    "                                          cv.COLORMAP_JET))\n",
    "\n",
    "Image.fromarray(heatmap0).save(\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/dataheatmap_frontal.jpg\")\n",
    "Image.fromarray(heatmap1).save(\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/dataheatmap_lateral.jpg\")\n",
    "img0=img0.cpu().numpy().astype(np.uint8).squeeze()\n",
    "img1=img1.cpu().numpy().astype(np.uint8).squeeze()\n",
    "img0 = cv.cvtColor(img0,cv.COLOR_GRAY2RGB)\n",
    "img1=cv.cvtColor(img1,cv.COLOR_GRAY2RGB)\n",
    "heatmap0 = cv.addWeighted(heatmap0, 0.5, img0, 0.5, 0)\n",
    "heatmap1 = cv.addWeighted(heatmap1, 0.5, img1, 0.5, 0)\n",
    "plt.imshow(heatmap0.squeeze())\n",
    "plt.show()\n",
    "plt.imshow(heatmap1.squeeze())\n",
    "plt.show()\n",
    "labels=torch.zeros_like(outputs).to(device)\n",
    "#loss = criterion(outputs.squeeze(), labels)\n",
    "outputs = torch.sigmoid(outputs)\n",
    "\n",
    "i+=1\n",
    "plt.imshow(inputs.squeeze().numpy())\n",
    "data=pd.DataFrame([outputs.detach().numpy().squeeze(),labels.numpy().squeeze()],columns=names,index=[\"preds\",\"ground-truth\"])\n",
    "print(data.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 6.00 GiB total capacity; 5.04 GiB already allocated; 0 bytes free; 5.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 45\u001B[0m\n\u001B[1;32m     42\u001B[0m targets \u001B[38;5;241m=\u001B[39m [ClassifierOutputTarget(i) \u001B[38;5;28;01mfor\u001B[39;00m i,output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(outputs\u001B[38;5;241m.\u001B[39msqueeze()\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mtolist()) \u001B[38;5;28;01mif\u001B[39;00m output\u001B[38;5;241m>\u001B[39m\u001B[38;5;241m0.5\u001B[39m]\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m ex,cam \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(cams,start\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m) :\n\u001B[0;32m---> 45\u001B[0m     heatmap0 \u001B[38;5;241m=\u001B[39m \u001B[43mcam\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg0\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfloat\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43mtargets\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtargets\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39msqueeze() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m255\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m255\u001B[39m\n\u001B[1;32m     46\u001B[0m     heatmap1 \u001B[38;5;241m=\u001B[39m cam(img1\u001B[38;5;241m.\u001B[39mfloat(),targets\u001B[38;5;241m=\u001B[39mtargets)\u001B[38;5;241m.\u001B[39msqueeze() \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m255\u001B[39m\u001B[38;5;241m+\u001B[39m\u001B[38;5;241m255\u001B[39m\n\u001B[1;32m     48\u001B[0m     heatmap0 \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mwhere(heatmap0\u001B[38;5;241m<\u001B[39m\u001B[38;5;241m0.9\u001B[39m,\u001B[38;5;241m0\u001B[39m,heatmap0)\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:188\u001B[0m, in \u001B[0;36mBaseCAM.__call__\u001B[0;34m(self, input_tensor, targets, aug_smooth, eigen_smooth)\u001B[0m\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m aug_smooth \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mTrue\u001B[39;00m:\n\u001B[1;32m    185\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_augmentation_smoothing(\n\u001B[1;32m    186\u001B[0m         input_tensor, targets, eigen_smooth)\n\u001B[0;32m--> 188\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    189\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meigen_smooth\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:95\u001B[0m, in \u001B[0;36mBaseCAM.forward\u001B[0;34m(self, input_tensor, targets, eigen_smooth)\u001B[0m\n\u001B[1;32m     84\u001B[0m     loss\u001B[38;5;241m.\u001B[39mbackward(retain_graph\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     86\u001B[0m \u001B[38;5;66;03m# In most of the saliency attribution papers, the saliency is\u001B[39;00m\n\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# computed with a single target layer.\u001B[39;00m\n\u001B[1;32m     88\u001B[0m \u001B[38;5;66;03m# Commonly it is the last convolutional layer.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     93\u001B[0m \u001B[38;5;66;03m# use all conv layers for example, all Batchnorm layers,\u001B[39;00m\n\u001B[1;32m     94\u001B[0m \u001B[38;5;66;03m# or something else.\u001B[39;00m\n\u001B[0;32m---> 95\u001B[0m cam_per_layer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_cam_per_layer\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     96\u001B[0m \u001B[43m                                           \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     97\u001B[0m \u001B[43m                                           \u001B[49m\u001B[43meigen_smooth\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maggregate_multi_layers(cam_per_layer)\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:127\u001B[0m, in \u001B[0;36mBaseCAM.compute_cam_per_layer\u001B[0;34m(self, input_tensor, targets, eigen_smooth)\u001B[0m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m i \u001B[38;5;241m<\u001B[39m \u001B[38;5;28mlen\u001B[39m(grads_list):\n\u001B[1;32m    125\u001B[0m     layer_grads \u001B[38;5;241m=\u001B[39m grads_list[i]\n\u001B[0;32m--> 127\u001B[0m cam \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_cam_image\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    128\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mtarget_layer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    129\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    130\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mlayer_activations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    131\u001B[0m \u001B[43m                         \u001B[49m\u001B[43mlayer_grads\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    132\u001B[0m \u001B[43m                         \u001B[49m\u001B[43meigen_smooth\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    133\u001B[0m cam \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mmaximum(cam, \u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m    134\u001B[0m scaled \u001B[38;5;241m=\u001B[39m scale_cam_image(cam, target_size)\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/pytorch_grad_cam/base_cam.py:50\u001B[0m, in \u001B[0;36mBaseCAM.get_cam_image\u001B[0;34m(self, input_tensor, target_layer, targets, activations, grads, eigen_smooth)\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_cam_image\u001B[39m(\u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     43\u001B[0m                   input_tensor: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m     44\u001B[0m                   target_layer: torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mModule,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     47\u001B[0m                   grads: torch\u001B[38;5;241m.\u001B[39mTensor,\n\u001B[1;32m     48\u001B[0m                   eigen_smooth: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m np\u001B[38;5;241m.\u001B[39mndarray:\n\u001B[0;32m---> 50\u001B[0m     weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_cam_weights\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     51\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mtarget_layer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     52\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mtargets\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     53\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mactivations\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[43m                                   \u001B[49m\u001B[43mgrads\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     55\u001B[0m     weighted_activations \u001B[38;5;241m=\u001B[39m weights[:, :, \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m activations\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m eigen_smooth:\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/pytorch_grad_cam/score_cam.py:55\u001B[0m, in \u001B[0;36mScoreCAM.get_cam_weights\u001B[0;34m(self, input_tensor, target_layer, targets, activations, grads)\u001B[0m\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tqdm\u001B[38;5;241m.\u001B[39mtqdm(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, tensor\u001B[38;5;241m.\u001B[39msize(\u001B[38;5;241m0\u001B[39m), BATCH_SIZE)):\n\u001B[1;32m     53\u001B[0m         batch \u001B[38;5;241m=\u001B[39m tensor[i: i \u001B[38;5;241m+\u001B[39m BATCH_SIZE, :]\n\u001B[1;32m     54\u001B[0m         outputs \u001B[38;5;241m=\u001B[39m [target(o)\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mitem()\n\u001B[0;32m---> 55\u001B[0m                    \u001B[38;5;28;01mfor\u001B[39;00m o \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbatch\u001B[49m\u001B[43m)\u001B[49m]\n\u001B[1;32m     56\u001B[0m         scores\u001B[38;5;241m.\u001B[39mextend(outputs)\n\u001B[1;32m     57\u001B[0m scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor(scores)\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m/mnt/f/IA-med_img/CheXpert2/models/CNN.py:125\u001B[0m, in \u001B[0;36mCNN.forward\u001B[0;34m(self, images)\u001B[0m\n\u001B[1;32m    121\u001B[0m outputs \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mzeros((images\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_classes))\u001B[38;5;241m.\u001B[39mto(images\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m images\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m] \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchannels :\n\u001B[0;32m--> 125\u001B[0m     outputs \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mweighted_forward(images) \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprob_pool \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    126\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m :\n\u001B[1;32m    127\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m2\u001B[39m) :\n\u001B[1;32m    128\u001B[0m         \u001B[38;5;66;03m#iterate through the two images for one patient\u001B[39;00m\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/timm/models/convnext.py:411\u001B[0m, in \u001B[0;36mConvNeXt.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    410\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[0;32m--> 411\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    412\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mforward_head(x)\n\u001B[1;32m    413\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/timm/models/convnext.py:398\u001B[0m, in \u001B[0;36mConvNeXt.forward_features\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    396\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward_features\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m    397\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstem(x)\n\u001B[0;32m--> 398\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstages\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    399\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm_pre(x)\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/timm/models/convnext.py:250\u001B[0m, in \u001B[0;36mConvNeXtStage.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    248\u001B[0m     x \u001B[38;5;241m=\u001B[39m checkpoint_seq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mblocks, x)\n\u001B[1;32m    249\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 250\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mblocks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    251\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/container.py:139\u001B[0m, in \u001B[0;36mSequential.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[0;32m--> 139\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m    140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/timm/models/convnext.py:184\u001B[0m, in \u001B[0;36mConvNeXtBlock.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m    182\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    183\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(x)\n\u001B[0;32m--> 184\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmlp\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    185\u001B[0m     x \u001B[38;5;241m=\u001B[39m x\u001B[38;5;241m.\u001B[39mpermute(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m3\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[1;32m    186\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgamma \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/timm/models/layers/mlp.py:28\u001B[0m, in \u001B[0;36mMlp.forward\u001B[0;34m(self, x)\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m     27\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc1(x)\n\u001B[0;32m---> 28\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mact\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdrop1(x)\n\u001B[1;32m     30\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfc2(x)\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/.virtualenvs/IA-med_img/lib/python3.9/site-packages/timm/models/layers/activations.py:145\u001B[0m, in \u001B[0;36mGELU.forward\u001B[0;34m(self, input)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: torch\u001B[38;5;241m.\u001B[39mTensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m torch\u001B[38;5;241m.\u001B[39mTensor:\n\u001B[0;32m--> 145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgelu\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 288.00 MiB (GPU 0; 6.00 GiB total capacity; 5.04 GiB already allocated; 0 bytes free; 5.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 1000x700 with 0 Axes>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#inputs,labels=test_dataset[i]\n",
    "#Manually loading images\n",
    "import cv2 as cv\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam import FullGrad,EigenCAM,GradCAM,HiResCAM,GradCAMPlusPlus,ScoreCAM\n",
    "from PIL import Image\n",
    "\n",
    "cams=[\n",
    "    #FullGrad(model,target_layers=[]),\n",
    "    #EigenCAM(model,target_layers=[model.backbone.stages[-1]]),\n",
    "    #GradCAM(model,target_layers=[model.backbone.stages[-1]]),\n",
    "    #HiResCAM(model,target_layers=[model.backbone.stages[-1]]),\n",
    "    ScoreCAM(model,target_layers=[model.backbone.stages[-1]],use_cuda=True),\n",
    "]\n",
    "img_size=512\n",
    "\n",
    "inputs,labels,idx = test_dataset[i]\n",
    "inputs = inputs[None,:,:,:]\n",
    "inputs = inputs.to(device,non_blocking=True)\n",
    "# forward + backward + optimize\n",
    "outputs = model(inputs)\n",
    "model.hierarchical=False\n",
    "channels=3\n",
    "i=0\n",
    "img0=inputs[0:1,i*channels : (i+1)*channels,:,:]\n",
    "i=1\n",
    "img1=inputs[0:1,i*channels : (i+1)*channels,:,:]\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "img = test_dataset.read_img(i)\n",
    "img0_display=img[:,:,i*channels : (i+1)*channels].astype(np.uint8)\n",
    "i=1\n",
    "img1_display=img[:,:,i*channels : (i+1)*channels].astype(np.uint8)\n",
    "if channels ==1 :\n",
    "    img0_display = cv.cvtColor(img0_display,cv.COLOR_GRAY2RGB)\n",
    "    img1_display=cv.cvtColor(img1_display,cv.COLOR_GRAY2RGB)\n",
    "\n",
    "rows=2\n",
    "columns=len(cams)\n",
    "\n",
    "targets = [ClassifierOutputTarget(i) for i,output in enumerate(outputs.squeeze().cpu().tolist()) if output>0.5]\n",
    "\n",
    "for ex,cam in enumerate(cams,start=1) :\n",
    "    heatmap0 = cam(img0.float(),targets=targets).squeeze() * -255+255\n",
    "    heatmap1 = cam(img1.float(),targets=targets).squeeze() * -255+255\n",
    "\n",
    "    heatmap0 = np.where(heatmap0<0.9,0,heatmap0)\n",
    "    heatmap1 = np.where(heatmap0<0.9,0,heatmap1)\n",
    "    fig.add_subplot(rows, columns, ex)\n",
    "    heatmap0 = np.array(cv.applyColorMap(cv.cvtColor(heatmap0[:, :,None].astype(np.uint8), cv.COLOR_RGB2BGR),cv.COLORMAP_JET))\n",
    "\n",
    "    heatmap0 = cv.addWeighted(heatmap0, 0.5, img0_display, 0.5, 0)\n",
    "    plt.imshow(heatmap0.squeeze())\n",
    "\n",
    "    fig.add_subplot(rows, columns, len(cams)+ex)\n",
    "    heatmap1 = np.array(cv.applyColorMap(cv.cvtColor(heatmap1[:, :,None].astype(np.uint8), cv.COLOR_RGB2BGR),cv.COLORMAP_JET))\n",
    "    heatmap1 = cv.addWeighted(heatmap1, 0.5, img1_display, 0.5, 0)\n",
    "    plt.imshow(heatmap1.squeeze())\n",
    "\n",
    "i+=1\n",
    "plt.show()\n",
    "plt.imshow(img0_display)\n",
    "plt.show()\n",
    "plt.imshow(img1_display)\n",
    "plt.show()\n",
    "#Image.fromarray(heatmap0).save(\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/dataheatmap_frontal.jpg\")\n",
    "#Image.fromarray(heatmap1).save(\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/dataheatmap_lateral.jpg\")\n",
    "\n",
    "labels=torch.zeros_like(outputs).to(device)\n",
    "#loss = criterion(outputs.squeeze(), labels)\n",
    "outputs = torch.sigmoid(outputs)\n",
    "\n",
    "data=pd.DataFrame([outputs.cpu().detach().numpy().squeeze(),labels.cpu().numpy().squeeze()],columns=names,index=[\"preds\",\"ground-truth\"])\n",
    "\n",
    "print(data.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
