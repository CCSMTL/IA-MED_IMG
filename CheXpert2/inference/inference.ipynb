{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#import from pypi\n",
    "import tqdm\n",
    "import torch\n",
    "import warnings\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "#local import\n",
    "from CheXpert2.models.CNN import CNN\n",
    "from CheXpert2.Metrics import Metrics\n",
    "from CheXpert2.dataloaders.CXRLoader import CXRLoader\n",
    "from CheXpert2 import names,hierarchy\n",
    "\n",
    "for key in hierarchy.keys():\n",
    "    if key not in names :\n",
    "        names.insert(0,key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    " def load_my_state_dict(self, state_dict):\n",
    "\n",
    "        own_state = self.state_dict()\n",
    "        for name, param in state_dict.items():\n",
    "            if name not in own_state:\n",
    "                 continue\n",
    "\n",
    "            own_state[name].copy_(param)\n",
    "\n",
    "def load_model() :\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda:0\"\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        warnings.warn(\"No gpu is available for the computation\")\n",
    "    models = [\n",
    "        CNN(\"convnext_base_384_in22ft1k\", channels=3, num_classes=len(names), pretrained=False,hierarchical=True),\n",
    "        #    CNN(\"convnext_base\", img_size=384, channels=1, num_classes=14, pretrained=False, pretraining=False),\n",
    "        #    CNN(\"densenet201\", img_size=384, channels=1, num_classes=14, pretrained=False, pretraining=False),\n",
    "        #    CNN(\"densenet201\", img_size=384, channels=1, num_classes=14, pretrained=False, pretraining=False),\n",
    "    ]\n",
    "    # model =  torch.nn.parallel.DistributedDataParallel(model)\n",
    "\n",
    "    # api = wandb.Api()\n",
    "    # run = api.run(f\"ccsmtl2/Chestxray/{args.run_id}\")\n",
    "    # run.file(\"models_weights/convnext_base/DistributedDataParallel.pt\").download(replace=True)\n",
    "    weights = [\n",
    "        \"/mnt/c/Users/joeda/PycharmProjects/IA-MED_IMG/data/model_weights/atomic-wildflower.pt\",\n",
    "        #    \"/data/home/jonathan/IA-MED_IMG/models_weights/convnext_base_2.pt\",\n",
    "        #    \"/data/home/jonathan/IA-MED_IMG/models_weights/densenet201.pt\",\n",
    "        #    \"/data/home/jonathan/IA-MED_IMG/models_weights/densenet201_2.pt\",\n",
    "    ]\n",
    "\n",
    "    for model, weight in zip(models, weights):\n",
    "        state_dict = torch.load(weight, map_location=torch.device(device))\n",
    "\n",
    "\n",
    "        #model.load_state_dict(state_dict)\n",
    "        load_my_state_dict(model,state_dict)\n",
    "        model.eval()\n",
    "        model = model.to(device)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_loop(model, loader, criterion, device):\n",
    "    \"\"\"\n",
    "\n",
    "    :param model: model to evaluate\n",
    "    :param loader: dataset loader\n",
    "    :param criterion: criterion to evaluate the loss\n",
    "    :param device: device to do the computation on\n",
    "    :return: val_loss for the N epoch, tensor of concatenated labels and predictions\n",
    "    \"\"\"\n",
    "    running_loss = 0\n",
    "    results = [torch.tensor([]), torch.tensor([])]\n",
    "\n",
    "    for inputs, labels,idx in tqdm.tqdm(loader):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "\n",
    "        inputs, labels = (\n",
    "            inputs.to(device, non_blocking=True),\n",
    "            labels.to(device, non_blocking=True),\n",
    "        )\n",
    "        #inputs,labels = loader.dataset.advanced_transform((inputs, labels))\n",
    "        # forward + backward + optimize\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        outputs = torch.sigmoid(outputs)\n",
    "        running_loss += loss.detach()\n",
    "\n",
    "        if inputs.shape != labels.shape:  # prevent storing images if training unets\n",
    "            results[1] = torch.cat(\n",
    "                (results[1], outputs.detach().cpu()), dim=0\n",
    "            )\n",
    "            results[0] = torch.cat((results[0], labels.cpu()), dim=0)\n",
    "\n",
    "        del (\n",
    "            inputs,\n",
    "            labels,\n",
    "            outputs,\n",
    "            loss,\n",
    "        )  # garbage management sometimes fails with cuda\n",
    "\n",
    "\n",
    "    return running_loss, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CRITICAL:root:Server not available ; switching offline\n",
      "/mnt/c/Users/joeda/PycharmProjects/IA-MED_IMG/CheXpert2/dataloaders/CXRLoader.py:184: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  data = data.groupby(\"Exam ID\").mean().round(0)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda:0\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    warnings.warn(\"No gpu is available for the computation\")\n",
    "\n",
    "# ----- parsing arguments -------------------------------------\n",
    "#start = torch.cuda.Event(enable_timing=True)\n",
    "#end = torch.cuda.Event(enable_timing=True)\n",
    "# ------loading test set --------------------------------------\n",
    "#img_dir = os.environ[\"img_dir\"]\n",
    "img_dir = \"/mnt/e\"\n",
    "os.environ[\"DEBUG\"]=\"True\"\n",
    "test_dataset = CXRLoader(\"Valid\",img_dir, img_size=384,channels=3,datasets=[\"ChexPert\"],prob=[0,]*5)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=2,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")\n",
    "# ----------------loading model -------------------------------\n",
    "\n",
    "model=load_model()\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:06<00:00, 16.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.072153806686401\n",
      "(200, 10)\n",
      "{'auc': {'Enlarged Cardiomediastinum': 0.5189974937343359, 'Cardiomegaly': 0.7340569877883311, 'Pleural Effusion': 0.9136029411764706, 'Pneumothorax': 0.802405498281787, 'Lung Opacity': 0.8957307060755336, 'Atelectasis': 0.8273066666666666, 'Pneumonia': 0.923828125, 'Consolidation': 0.8688616071428572, 'Edema': 0.8738698010849909, 'No Finding': 0.7981874447391689, 'mean': 0.8156847271690142}, 'f1': {'Enlarged Cardiomediastinum': 0.6885245901639345, 'Cardiomegaly': 0.5405405405405406, 'Pleural Effusion': 0.6936416184971099, 'Pneumothorax': 0.072992700729927, 'Lung Opacity': 0.7862068965517242, 'Atelectasis': 0.6075949367088608, 'Pneumonia': 0.0837696335078534, 'Consolidation': 0.3106796116504854, 'Edema': 0.3980582524271844, 'No Finding': 0.9304812834224598, 'mean': 0.511249006420008}, 'recall': {'Enlarged Cardiomediastinum': 1.0, 'Cardiomegaly': 0.6060606060606061, 'Pleural Effusion': 0.9375, 'Pneumothorax': 0.8333333333333334, 'Lung Opacity': 0.9827586206896551, 'Atelectasis': 0.96, 'Pneumonia': 1.0, 'Consolidation': 1.0, 'Edema': 0.9761904761904762, 'No Finding': 1.0}, 'precision': {'Enlarged Cardiomediastinum': 0.525, 'Cardiomegaly': 0.4878048780487805, 'Pleural Effusion': 0.5504587155963303, 'Pneumothorax': 0.03816793893129771, 'Lung Opacity': 0.6551724137931034, 'Atelectasis': 0.4444444444444444, 'Pneumonia': 0.04371584699453552, 'Consolidation': 0.1839080459770115, 'Edema': 0.25, 'No Finding': 0.87}, 'accuracy': 2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#start.record()\n",
    "import time\n",
    "start = time.time()\n",
    "running_loss, results = infer_loop(model=model, loader=test_loader, criterion=criterion, device=device)\n",
    "#end.record()\n",
    "end=time.time()\n",
    "#torch.cuda.synchronize()\n",
    "#print(\"time : \", start.elapsed_time(end))\n",
    "print(end-start)\n",
    "#plt.imshow(np.sum(heatmaps[0][0].detach().cpu().numpy(), axis=0))\n",
    "#plt.savefig(\"heatmaps.png\")\n",
    "\n",
    "metric = Metrics(num_classes=len(names), names=names, threshold=np.zeros((len(names))) + 0.5)\n",
    "metrics = metric.metrics()\n",
    "metrics_results = {}\n",
    "for key in metrics:\n",
    "    pred = results[1].numpy()\n",
    "    true = results[0].numpy().round(0)\n",
    "    metric_result = metrics[key](true, pred)\n",
    "    metrics_results[key] = metric_result\n",
    "\n",
    "print(metrics_results)\n",
    "i=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_1936/3971977836.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0mpath0\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/data/1808916906062113_view0001.jpg\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[0mpath1\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/data/1808916906062113_view0002.jpg\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 14\u001B[0;31m \u001B[0;32massert\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpath0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     15\u001B[0m \u001B[0mimage0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{img_dir}{path0}\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIMREAD_GRAYSCALE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mimg_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0mimage1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimread\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{img_dir}{path1}\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mIMREAD_GRAYSCALE\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg_size\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mimg_size\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "FOR A SPECIFIC IMAGE\n",
    "\"\"\"\n",
    "#inputs,labels=test_dataset[i]\n",
    "#Manually loading images\n",
    "import cv2 as cv\n",
    "from pytorch_grad_cam import FullGrad\n",
    "from PIL import Image\n",
    "cam=FullGrad(model,target_layers=[])\n",
    "img_size=384\n",
    "img_dir=\"\"\n",
    "path0=\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/data/1808916906062113_view0001.jpg\"\n",
    "path1=\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/data/1808916906062113_view0002.jpg\"\n",
    "assert os.path.exists(path0)\n",
    "image0 = np.array(cv.resize(cv.imread(f\"{img_dir}{path0}\", cv.IMREAD_GRAYSCALE),(img_size,img_size)))\n",
    "image1 = np.array(cv.resize(cv.imread(f\"{img_dir}{path1}\", cv.IMREAD_GRAYSCALE),(img_size,img_size)))\n",
    "inputs = np.concatenate([image0[None,:,:],image1[None,:,:]],axis=0)\n",
    "inputs=inputs[None,:,:,:]\n",
    "# inputs, labels = (\n",
    "#             inputs.to(device, non_blocking=True),\n",
    "#             labels.to(device, non_blocking=True),\n",
    "#         )\n",
    "\n",
    "\n",
    "inputs = torch.from_numpy(inputs).to(device,non_blocking=True)\n",
    "# forward + backward + optimize\n",
    "outputs = model(inputs)\n",
    "img0=inputs[:,0:1,:,:]\n",
    "heatmap0 = cam(img0.float()).squeeze() * -255+255\n",
    "\n",
    "img1=inputs[:,1:2,:,:]\n",
    "heatmap1 = cam(img1.float()).squeeze() * -255+255\n",
    "\n",
    "\n",
    "heatmap0 = np.array(cv.applyColorMap(cv.cvtColor(heatmap0[:, :, None].astype(np.uint8), cv.COLOR_RGB2BGR),\n",
    "                                          cv.COLORMAP_JET))\n",
    "\n",
    "heatmap1 = np.array(cv.applyColorMap(cv.cvtColor(heatmap1[:, :, None].astype(np.uint8), cv.COLOR_RGB2BGR),\n",
    "                                          cv.COLORMAP_JET))\n",
    "\n",
    "Image.fromarray(heatmap0).save(\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/dataheatmap_frontal.jpg\")\n",
    "Image.fromarray(heatmap1).save(\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/dataheatmap_lateral.jpg\")\n",
    "img0=img0.cpu().numpy().astype(np.uint8).squeeze()\n",
    "img1=img1.cpu().numpy().astype(np.uint8).squeeze()\n",
    "img0 = cv.cvtColor(img0,cv.COLOR_GRAY2RGB)\n",
    "img1=cv.cvtColor(img1,cv.COLOR_GRAY2RGB)\n",
    "heatmap0 = cv.addWeighted(heatmap0, 0.5, img0, 0.5, 0)\n",
    "heatmap1 = cv.addWeighted(heatmap1, 0.5, img1, 0.5, 0)\n",
    "plt.imshow(heatmap0.squeeze())\n",
    "plt.show()\n",
    "plt.imshow(heatmap1.squeeze())\n",
    "plt.show()\n",
    "labels=torch.zeros_like(outputs).to(device)\n",
    "#loss = criterion(outputs.squeeze(), labels)\n",
    "outputs = torch.sigmoid(outputs)\n",
    "\n",
    "i+=1\n",
    "plt.imshow(inputs.squeeze().numpy())\n",
    "data=pd.DataFrame([outputs.detach().numpy().squeeze(),labels.numpy().squeeze()],columns=names,index=[\"preds\",\"ground-truth\"])\n",
    "print(data.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.7.0) /io/opencv/modules/core/src/arithm.cpp:647: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31merror\u001B[0m                                     Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_1936/2021844610.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     54\u001B[0m     \u001B[0mheatmap0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mapplyColorMap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcvtColor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mheatmap0\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m:\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mastype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0muint8\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCOLOR_RGB2BGR\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mCOLORMAP_JET\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     55\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 56\u001B[0;31m     \u001B[0mheatmap0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0maddWeighted\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mheatmap0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg0_display\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0.5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     57\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimshow\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mheatmap0\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msqueeze\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     58\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31merror\u001B[0m: OpenCV(4.7.0) /io/opencv/modules/core/src/arithm.cpp:647: error: (-209:Sizes of input arguments do not match) The operation is neither 'array op array' (where arrays have the same size and the same number of channels), nor 'array op scalar', nor 'scalar op array' in function 'arithm_op'\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAMQAAAEmCAYAAAA9XT9WAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAASnklEQVR4nO3dbWxTdf/H8U9301amLbJJGTDGQNgwRIQugw0nEaRkEBIeGDAmYxhIbIzCnKgdS8QRkgVvMCoMLsjAkExYYEBInEofwBiOB7J0RhkK4W4DN5YNPR0oHYzf/wH/9fqWdrpTWja4Pq/kPOjPc3p+OzlvTm+OqUEppUBEAICYgZ4A0WDCIIgEBkEkMAgigUEQCQyCSGAQRAKDIBIYBJHAIIgE3UEcO3YMCxcuxMiRI2EwGHDw4MF/3aa2thZ2ux1msxnjxo3D1q1bw5krUdTpDuLGjRuYMmUKNm3a1K/1L1y4gPnz5yM3Nxcejwdr1qzBypUrUV1drXuyRNFmuJ+b+wwGAw4cOIBFixb1uc7777+PQ4cO4fTp0/4xp9OJn376CSdOnAh310RRERftHZw4cQIOhyNgbN68eaioqMCtW7cQHx8ftI3P54PP5/M/vnPnDq5du4bExEQYDIZoT5keAkopdHV1YeTIkYiJidxb4agH0dbWBpvNFjBms9lw+/ZtdHR0IDk5OWibsrIylJaWRntq9AhoaWnB6NGjI/Z8UQ8CQNC/6r2v0vr61764uBhFRUX+x5qmYcyYMWhpaYHFYoneROmh4fV6kZKSgieeeCKizxv1IEaMGIG2traAsfb2dsTFxSExMTHkNiaTCSaTKWjcYrEwCAoQ6ZfQUf8eIjs7G263O2Ds8OHDyMzMDPn+gWgg6Q7i+vXraGxsRGNjI4C7H6s2NjaiubkZwN2XO0uXLvWv73Q6cenSJRQVFeH06dPYsWMHKioqsHr16sj8BUSRpHQ6cuSIAhC0FBQUKKWUKigoULNmzQrY5ujRo2rq1KnKaDSqsWPHqi1btujap6ZpCoDSNE3vdOkRFa1z4r6+h3hQvF4vrFYrNE3jewgCEL1zgvcyEQkMgkhgEEQCgyASGASRwCCIBAZBJDAIIoFBEAkMgkhgEEQCgyASGASRwCCIBAZBJDAIIoFBEAkMgkhgEEQCgyASGASRwCCIBAZBJDAIIoFBEAkMgkhgEEQCgyASGASRwCCIBAZBJDAIIoFBEAkMgkhgEEQCgyASGASRwCCIhLCCKC8vR1paGsxmM+x2O+rq6v5x/crKSkyZMgVDhgxBcnIyXnvtNXR2doY1YaJo0h1EVVUVCgsLUVJSAo/Hg9zcXOTl5aG5uTnk+sePH8fSpUuxfPlynDp1Cnv37sWPP/6IFStW3PfkiSJO7y+9Z2VlKafTGTCWkZGhXC5XyPU//vhjNW7cuICxL774Qo0ePbrf+4zWr9bTwyta54SuK0R3dzcaGhrgcDgCxh0OB+rr60Nuk5OTg8uXL6OmpgZKKVy9ehX79u3DggUL+tyPz+eD1+sNWIgeBF1BdHR0oKenBzabLWDcZrOhra0t5DY5OTmorKzEkiVLYDQaMWLECAwdOhRffvlln/spKyuD1Wr1LykpKXqmSRS2sN5UGwyGgMdKqaCxXk1NTVi5ciU++OADNDQ04LvvvsOFCxfgdDr7fP7i4mJomuZfWlpawpkmkW5xelZOSkpCbGxs0NWgvb096KrRq6ysDDNnzsS7774LAHj22WeRkJCA3NxcrF+/HsnJyUHbmEwmmEwmPVMjighdVwij0Qi73Q632x0w7na7kZOTE3Kbv/76CzExgbuJjY0FcPfKQjSo6H0XvmfPHhUfH68qKipUU1OTKiwsVAkJCerixYtKKaVcLpfKz8/3r79z504VFxenysvL1blz59Tx48dVZmamysrK6vc++SkT3Sta54Sul0wAsGTJEnR2dmLdunVobW3F5MmTUVNTg9TUVABAa2trwHcSy5YtQ1dXFzZt2oR33nkHQ4cOxezZs7Fhw4ZINU0UMQalBv/rFq/XC6vVCk3TYLFYBno6NAhE65zgvUxEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCIhrCDKy8uRlpYGs9kMu92Ourq6f1zf5/OhpKQEqampMJlMGD9+PHbs2BHWhImiSfcPt1dVVaGwsBDl5eWYOXMm/vOf/yAvLw9NTU0YM2ZMyG0WL16Mq1evoqKiAk8//TTa29tx+/bt+548UcQpnbKyspTT6QwYy8jIUC6XK+T63377rbJaraqzs1Pvrvw0TVMAlKZpYT8HPVqidU7oesnU3d2NhoYGOByOgHGHw4H6+vqQ2xw6dAiZmZn46KOPMGrUKEycOBGrV6/G33//3ed+fD4fvF5vwEL0IOh6ydTR0YGenh7YbLaAcZvNhra2tpDbnD9/HsePH4fZbMaBAwfQ0dGBN954A9euXevzfURZWRlKS0v1TI0oIsJ6U20wGAIeK6WCxnrduXMHBoMBlZWVyMrKwvz587Fx40Z89dVXfV4liouLoWmaf2lpaQlnmkS66bpCJCUlITY2Nuhq0N7eHnTV6JWcnIxRo0bBarX6xyZNmgSlFC5fvowJEyYEbWMymWAymfRMjSgidF0hjEYj7HY73G53wLjb7UZOTk7IbWbOnInff/8d169f94+dOXMGMTExGD16dBhTJooive/C9+zZo+Lj41VFRYVqampShYWFKiEhQV28eFEppZTL5VL5+fn+9bu6utTo0aPVyy+/rE6dOqVqa2vVhAkT1IoVK/q9T37KRPeK1jmh+3uIJUuWoLOzE+vWrUNraysmT56MmpoapKamAgBaW1vR3NzsX//xxx+H2+3GW2+9hczMTCQmJmLx4sVYv359pJomihiDUkoN9CT+jdfrhdVqhaZpsFgsAz0dGgSidU7wXiYigUEQCQyCSGAQRAKDIBIYBJHAIIgEBkEkMAgigUEQCQyCSGAQRAKDIBIYBJHAIIgEBkEkMAgigUEQCQyCSGAQRAKDIBIYBJHAIIgEBkEkMAgigUEQCQyCSGAQRAKDIBIYBJHAIIgEBkEkMAgigUEQCQyCSGAQRAKDIBIYBJEQVhDl5eVIS0uD2WyG3W5HXV1dv7b74YcfEBcXh+eeey6c3RJFne4gqqqqUFhYiJKSEng8HuTm5iIvLy/gx9pD0TQNS5cuxZw5c8KeLFG06f7h9unTp2PatGnYsmWLf2zSpElYtGgRysrK+tzulVdewYQJExAbG4uDBw+isbGx3/vkD7fTvQbFD7d3d3ejoaEBDocjYNzhcKC+vr7P7Xbu3Ilz585h7dq1/dqPz+eD1+sNWIgeBF1BdHR0oKenBzabLWDcZrOhra0t5DZnz56Fy+VCZWUl4uLi+rWfsrIyWK1W/5KSkqJnmkRhC+tNtcFgCHislAoaA4Cenh68+uqrKC0txcSJE/v9/MXFxdA0zb+0tLSEM00i3fr3T/b/S0pKQmxsbNDVoL29PeiqAQBdXV04efIkPB4P3nzzTQDAnTt3oJRCXFwcDh8+jNmzZwdtZzKZYDKZ9EyNKCJ0XSGMRiPsdjvcbnfAuNvtRk5OTtD6FosFP//8MxobG/2L0+lEeno6GhsbMX369PubPVGE6bpCAEBRURHy8/ORmZmJ7OxsbNu2Dc3NzXA6nQDuvty5cuUKdu3ahZiYGEyePDlg++HDh8NsNgeNEw0GuoNYsmQJOjs7sW7dOrS2tmLy5MmoqalBamoqAKC1tfVfv5MgGqx0fw8xEPg9BN1rUHwPQfSoYxBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSSEFUR5eTnS0tJgNptht9tRV1fX57r79+/H3Llz8dRTT8FisSA7Oxvff/992BMmiibdQVRVVaGwsBAlJSXweDzIzc1FXl5enz/WfuzYMcydOxc1NTVoaGjAiy++iIULF8Lj8dz35IkiTumUlZWlnE5nwFhGRoZyuVz9fo5nnnlGlZaW9nt9TdMUAKVpWr+3oUdbtM4JXVeI7u5uNDQ0wOFwBIw7HA7U19f36znu3LmDrq4uDBs2rM91fD4fvF5vwEL0IOgKoqOjAz09PbDZbAHjNpsNbW1t/XqOTz/9FDdu3MDixYv7XKesrAxWq9W/pKSk6JkmUdjCelNtMBgCHiulgsZC2b17Nz788ENUVVVh+PDhfa5XXFwMTdP8S0tLSzjTJNItTs/KSUlJiI2NDboatLe3B1017lVVVYXly5dj7969eOmll/5xXZPJBJPJpGdqRBGh6wphNBpht9vhdrsDxt1uN3Jycvrcbvfu3Vi2bBm+/vprLFiwILyZEj0Auq4QAFBUVIT8/HxkZmYiOzsb27ZtQ3NzM5xOJ4C7L3euXLmCXbt2Abgbw9KlS/H5559jxowZ/qvLY489BqvVGsE/hSgCwvloavPmzSo1NVUZjUY1bdo0VVtb6/9vBQUFatasWf7Hs2bNUgCCloKCgn7vjx+70r2idU4YlFJqAHvsF6/XC6vVCk3TYLFYBno6NAhE65zgvUxEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCKBQRAJDIJIYBBEAoMgEhgEkcAgiAQGQSQwCCIhrCDKy8uRlpYGs9kMu92Ourq6f1y/trYWdrsdZrMZ48aNw9atW8OaLFG06Q6iqqoKhYWFKCkpgcfjQW5uLvLy8tDc3Bxy/QsXLmD+/PnIzc2Fx+PBmjVrsHLlSlRXV9/35IkiTu8vvWdlZSmn0xkwlpGRoVwuV8j133vvPZWRkREw9vrrr6sZM2b0e5/R+tV6enhF65yI0xNPd3c3Ghoa4HK5AsYdDgfq6+tDbnPixAk4HI6AsXnz5qGiogK3bt1CfHx80DY+nw8+n8//WNM0AHd/vZ4I+O+5oJSK6PPqCqKjowM9PT2w2WwB4zabDW1tbSG3aWtrC7n+7du30dHRgeTk5KBtysrKUFpaGjSekpKiZ7r0P6CzsxNWqzViz6criF4GgyHgsVIqaOzf1g813qu4uBhFRUX+x3/++SdSU1PR3Nwc0T/+YeT1epGSkoKWlhZYLJaBns6A0TQNY8aMwbBhwyL6vLqCSEpKQmxsbNDVoL29Pegq0GvEiBEh14+Li0NiYmLIbUwmE0wmU9C41Wr9nz4JJIvFwmMBICYmst8c6Ho2o9EIu90Ot9sdMO52u5GTkxNym+zs7KD1Dx8+jMzMzJDvH4gGlN534Xv27FHx8fGqoqJCNTU1qcLCQpWQkKAuXryolFLK5XKp/Px8//rnz59XQ4YMUW+//bZqampSFRUVKj4+Xu3bt6/f++SnTP/FY3FXtI6D7iCUUmrz5s0qNTVVGY1GNW3aNFVbW+v/bwUFBWrWrFkB6x89elRNnTpVGY1GNXbsWLVlyxZd+7t586Zau3atunnzZjjTfaTwWNwVreNgUCrCn1sRPcR4LxORwCCIBAZBJDAIImHQBMFbyu/ScxyOHj0Kg8EQtPz6668PcMbRcezYMSxcuBAjR46EwWDAwYMH/3WbiJwTEf3MKky9321s375dNTU1qVWrVqmEhAR16dKlkOv3frexatUq1dTUpLZv3677u43BSO9xOHLkiAKgfvvtN9Xa2upfbt++/YBnHnk1NTWqpKREVVdXKwDqwIED/7h+pM6JQRHEQNxSPhjpPQ69Qfzxxx8PYHYDpz9BROqcGPCXTL23lN97i3g4t5SfPHkSt27ditpcoymc49Br6tSpSE5Oxpw5c3DkyJFoTnPQitQ5MeBBROOW8odROMchOTkZ27ZtQ3V1Nfbv34/09HTMmTMHx44dexBTHlQidU6Edft3NET7lvKHhZ7jkJ6ejvT0dP/j7OxstLS04JNPPsELL7wQ1XkORpE4Jwb8CvGgbikf7MI5DqHMmDEDZ8+ejfT0Br1InRMDHgRvKb8rnOMQisfjCfl/IT7qInZO6HoLHiUDcUv5YKT3OHz22WfqwIED6syZM+qXX35RLpdLAVDV1dUD9SdETFdXl/J4PMrj8SgAauPGjcrj8fg/go7WOTEoglDqwd9SPljpOQ4bNmxQ48ePV2azWT355JPq+eefV998880AzDryej9SvncpKChQSkXvnODt30TCgL+HIBpMGASRwCCIBAZBJDAIIoFBEAkMgkhgEEQCgyASGASRwCCIBAZBJPwfGcOV7oyw2ykAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1000x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#inputs,labels=test_dataset[i]\n",
    "#Manually loading images\n",
    "import cv2 as cv\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam import FullGrad,EigenCAM,GradCAM,HiResCAM,GradCAMPlusPlus,ScoreCAM\n",
    "from PIL import Image\n",
    "\n",
    "cams=[\n",
    "    FullGrad(model,target_layers=[]),\n",
    "    EigenCAM(model,target_layers=[model.backbone.stages[-1]]),\n",
    "    GradCAM(model,target_layers=[model.backbone.stages[-1]]),\n",
    "    HiResCAM(model,target_layers=[model.backbone.stages[-1]]),\n",
    "    ScoreCAM(model,target_layers=[model.backbone.stages[-1]],use_cuda=True),\n",
    "]\n",
    "setattr(cams[-1],\"batch_size\",16)\n",
    "img_size=384\n",
    "\n",
    "inputs,labels,idx = test_dataset[i]\n",
    "inputs = inputs[None,:,:,:]\n",
    "inputs = inputs.to(device,non_blocking=True)\n",
    "# forward + backward + optimize\n",
    "outputs = model(inputs)\n",
    "model.hierarchical=False\n",
    "channels=3\n",
    "i=0\n",
    "img0=inputs[0:1,i*channels : (i+1)*channels,:,:]\n",
    "i=1\n",
    "img1=inputs[0:1,i*channels : (i+1)*channels,:,:]\n",
    "# create figure\n",
    "fig = plt.figure(figsize=(10, 7))\n",
    "\n",
    "img = test_dataset.read_img(i)\n",
    "img0_display=img[:,:,0].astype(np.uint8)\n",
    "i=1\n",
    "img1_display=img[:,:,3].astype(np.uint8)\n",
    "if channels ==1 :\n",
    "    img0_display = cv.cvtColor(img0_display,cv.COLOR_GRAY2RGB)\n",
    "    img1_display=cv.cvtColor(img1_display,cv.COLOR_GRAY2RGB)\n",
    "\n",
    "rows=2\n",
    "columns=len(cams)\n",
    "\n",
    "targets = [ClassifierOutputTarget(i) for i,output in enumerate(outputs.squeeze().cpu().tolist()) if output>0.5]\n",
    "\n",
    "for ex,cam in enumerate(cams,start=1) :\n",
    "    heatmap0 = cam(img0.float(),targets=targets).squeeze() * -255+255\n",
    "    del img0\n",
    "    heatmap1 = cam(img1.float(),targets=targets).squeeze() * -255+255\n",
    "    del img1\n",
    "\n",
    "    heatmap0 = np.where(heatmap0<0.9,0,heatmap0)\n",
    "    heatmap1 = np.where(heatmap0<0.9,0,heatmap1)\n",
    "    fig.add_subplot(rows, columns, ex)\n",
    "    heatmap0 = np.array(cv.applyColorMap(cv.cvtColor(heatmap0[:, :,None].astype(np.uint8), cv.COLOR_RGB2BGR),cv.COLORMAP_JET))\n",
    "\n",
    "    heatmap0 = cv.addWeighted(heatmap0, 0.5, img0_display, 0.5, 0)\n",
    "    plt.imshow(heatmap0.squeeze())\n",
    "\n",
    "    fig.add_subplot(rows, columns, len(cams)+ex)\n",
    "    heatmap1 = np.array(cv.applyColorMap(cv.cvtColor(heatmap1[:, :,None].astype(np.uint8), cv.COLOR_RGB2BGR),cv.COLORMAP_JET))\n",
    "    heatmap1 = cv.addWeighted(heatmap1, 0.5, img1_display, 0.5, 0)\n",
    "    plt.imshow(heatmap1.squeeze())\n",
    "\n",
    "i+=1\n",
    "plt.show()\n",
    "plt.imshow(img0_display)\n",
    "plt.show()\n",
    "plt.imshow(img1_display)\n",
    "plt.show()\n",
    "#Image.fromarray(heatmap0).save(\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/dataheatmap_frontal.jpg\")\n",
    "#Image.fromarray(heatmap1).save(\"c:/Users/joeda/PycharmProjects/IA-MED_IMG/dataheatmap_lateral.jpg\")\n",
    "\n",
    "labels=torch.zeros_like(outputs).to(device)\n",
    "#loss = criterion(outputs.squeeze(), labels)\n",
    "outputs = torch.sigmoid(outputs)\n",
    "\n",
    "data=pd.DataFrame([outputs.cpu().detach().numpy().squeeze(),labels.cpu().numpy().squeeze()],columns=names,index=[\"preds\",\"ground-truth\"])\n",
    "\n",
    "print(data.T)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
