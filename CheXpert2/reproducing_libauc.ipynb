{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Qkb6bYy3rOx"
   },
   "source": [
    "# **Optimizing Multi-label AUROC loss on Chest X-Ray Dataset (CheXpert)**\n",
    "\n",
    "**Author**: Zhuoning Yuan\n",
    "\n",
    "**Introduction**\n",
    "\n",
    "In this tutorial, you will learn how to quickly train a DenseNet121 model by optimizing AUROC using our novel AUCMLoss and PESG optimizer on Chest X-Ray dataset, e.g.,[CheXpert](https://stanfordmlgroup.github.io/competitions/chexpert/). After completion of this tutorial, you should be able to use LibAUC to train your own models on your own datasets.\n",
    "\n",
    "\n",
    "\n",
    "**Useful Resources**:\n",
    "* Website: https://libauc.org\n",
    "* Github: https://github.com/Optimization-AI/LibAUC\n",
    "\n",
    "**Reference**:  \n",
    "\n",
    "If you find this tutorial helpful in your work,  please acknowledge our library and cite the following paper:\n",
    "\n",
    "<pre>\n",
    "@inproceedings{yuan2021large,\n",
    "  title={Large-scale robust deep auc maximization: A new surrogate loss and empirical studies on medical image classification},\n",
    "  author={Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},\n",
    "  booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},\n",
    "  pages={3040--3049},\n",
    "  year={2021}\n",
    "}\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hTJ3ca0u4YQ4"
   },
   "source": [
    "# **Installing LibAUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "h8iVw1kU3guh"
   },
   "outputs": [],
   "source": [
    "#!pip install libauc==1.2.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nlD-4SrE4dVW"
   },
   "source": [
    "# **Downloading CheXpert**\n",
    " \n",
    "*   To request dataset access, you need to apply from CheXpert website: https://stanfordmlgroup.github.io/competitions/chexpert/\n",
    "*   In this tutorial, we use the smaller version of dataset with lower image resolution, i.e., *CheXpert-v1.0-small.zip*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "CcsJ4eoj3VST"
   },
   "outputs": [],
   "source": [
    "#!cp /content/drive/MyDrive/chexpert-dataset/CheXpert-v1.0-small.zip /content/\n",
    "#!mkdir CheXpert\n",
    "#!unzip CheXpert-v1.0-small.zip -d /content/CheXpert/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OVvrt3ku4qpq"
   },
   "source": [
    "\n",
    "# **Importing LibAUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XGHWer3v4qJo"
   },
   "outputs": [],
   "source": [
    "from libauc.losses import AUCM_MultiLabel, CrossEntropyLoss\n",
    "from libauc.optimizers import PESG, Adam\n",
    "from libauc.models import densenet121 as DenseNet121\n",
    "from libauc.datasets import CheXpert\n",
    "from libauc.metrics import auc_roc_score # for multi-task\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torch \n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2swK5Mo7Kca"
   },
   "source": [
    "# **Reproducibility**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OiiT5oEp7J3C"
   },
   "outputs": [],
   "source": [
    "def set_all_seeds(SEED):\n",
    "    # REPRODUCIBILITY\n",
    "    torch.manual_seed(SEED)\n",
    "    np.random.seed(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Bjd5Q5wkAT7"
   },
   "source": [
    "# **Datasets, Loss and Optimizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KkxJ2ZNNj_uN",
    "outputId": "93c1f0c9-c94f-4a39-dcba-e22e647c606f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.12241724991755092, 0.32190737435022276, 0.06796421448276946, 0.31190878776298636, 0.402555659671146]\n"
     ]
    }
   ],
   "source": [
    "root = '/mnt/c/Users/bejo2361/PycharmProjects/IA-MED_IMG/data/public_data/ChexPert/CheXpert-v1.0/'\n",
    "# Index=-1 denotes multi-label with 5 diseases\n",
    "traindSet = CheXpert(csv_path=root+'train.csv', image_root_path=root, use_upsampling=False, use_frontal=True, image_size=224, mode='train', class_index=-1, verbose=False)\n",
    "testSet =  CheXpert(csv_path=root+'valid.csv',  image_root_path=root, use_upsampling=False, use_frontal=True, image_size=224, mode='valid', class_index=-1, verbose=False)\n",
    "trainloader =  torch.utils.data.DataLoader(traindSet, batch_size=32, num_workers=2, shuffle=True)\n",
    "testloader =  torch.utils.data.DataLoader(testSet, batch_size=32, num_workers=2, shuffle=False)\n",
    "\n",
    "# check imbalance ratio for each task\n",
    "print (traindSet.imratio_list )\n",
    "\n",
    "# paramaters\n",
    "SEED = 123\n",
    "BATCH_SIZE = 32\n",
    "lr = 0.1 \n",
    "epoch_decay = 2e-3\n",
    "weight_decay = 1e-5\n",
    "margin = 1.0\n",
    "total_epochs = 2\n",
    "\n",
    "# model\n",
    "set_all_seeds(SEED)\n",
    "model = DenseNet121(pretrained=True, last_activation=None, activations='relu', num_classes=5)\n",
    "#model = model.cuda()\n",
    "\n",
    "# define loss & optimizer\n",
    "loss_fn = AUCM_MultiLabel(num_classes=5,device=\"cpu\")\n",
    "optimizer = PESG(model,\n",
    "                 device=\"cpu\",\n",
    "                 loss_fn=loss_fn,\n",
    "                 lr=lr, \n",
    "                 margin=margin, \n",
    "                 epoch_decay=epoch_decay, \n",
    "                 weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3Bge6KM7lBP"
   },
   "source": [
    "# **Multi-label Training**\n",
    "Optimizing Multi-label AUROC loss (e.g., 5 tasks)   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "u6p0oVY8AouP",
    "outputId": "01c2b44b-7c08-4a5e-8209-fceb75aab393"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Training\n",
      "------------------------------\n",
      "Epoch=0, BatchID=0, Val_AUC=0.5547, Best_Val_AUC=0.5547\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn [6], line 17\u001B[0m\n\u001B[1;32m     15\u001B[0m loss \u001B[38;5;241m=\u001B[39m loss_fn(y_pred, train_labels)\n\u001B[1;32m     16\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 17\u001B[0m loss\u001B[38;5;241m.\u001B[39mbackward()\n\u001B[1;32m     18\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mstep()\n\u001B[1;32m     20\u001B[0m \u001B[38;5;66;03m# validation  \u001B[39;00m\n",
      "File \u001B[0;32m~/.virtualenvs/IA-MED_IMG/lib/python3.9/site-packages/torch/_tensor.py:396\u001B[0m, in \u001B[0;36mTensor.backward\u001B[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001B[0m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m has_torch_function_unary(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    388\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m handle_torch_function(\n\u001B[1;32m    389\u001B[0m         Tensor\u001B[38;5;241m.\u001B[39mbackward,\n\u001B[1;32m    390\u001B[0m         (\u001B[38;5;28mself\u001B[39m,),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    394\u001B[0m         create_graph\u001B[38;5;241m=\u001B[39mcreate_graph,\n\u001B[1;32m    395\u001B[0m         inputs\u001B[38;5;241m=\u001B[39minputs)\n\u001B[0;32m--> 396\u001B[0m \u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mautograd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgradient\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/.virtualenvs/IA-MED_IMG/lib/python3.9/site-packages/torch/autograd/__init__.py:173\u001B[0m, in \u001B[0;36mbackward\u001B[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001B[0m\n\u001B[1;32m    168\u001B[0m     retain_graph \u001B[38;5;241m=\u001B[39m create_graph\n\u001B[1;32m    170\u001B[0m \u001B[38;5;66;03m# The reason we repeat same the comment below is that\u001B[39;00m\n\u001B[1;32m    171\u001B[0m \u001B[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001B[39;00m\n\u001B[1;32m    172\u001B[0m \u001B[38;5;66;03m# calls in the traceback and some print out the last line\u001B[39;00m\n\u001B[0;32m--> 173\u001B[0m \u001B[43mVariable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_execution_engine\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_backward\u001B[49m\u001B[43m(\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001B[39;49;00m\n\u001B[1;32m    174\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad_tensors_\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mretain_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcreate_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    175\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_unreachable\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccumulate_grad\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# training\n",
    "print ('Start Training')\n",
    "print ('-'*30)\n",
    "\n",
    "best_val_auc = 0 \n",
    "for epoch in range(total_epochs):\n",
    "    if epoch > 0:\n",
    "        optimizer.update_regularizer(decay_factor=10)    \n",
    "\n",
    "    for idx, data in enumerate(trainloader):\n",
    "      train_data, train_labels = data\n",
    "      #train_data, train_labels  = train_data.cuda(), train_labels.cuda()\n",
    "      y_pred = model(train_data)\n",
    "      y_pred = torch.sigmoid(y_pred)\n",
    "      loss = loss_fn(y_pred, train_labels)\n",
    "      optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "        \n",
    "      # validation  \n",
    "      if idx % 400 == 0:\n",
    "         model.eval()\n",
    "         with torch.no_grad():    \n",
    "              test_pred = []\n",
    "              test_true = [] \n",
    "              for jdx, data in enumerate(testloader):\n",
    "                  test_data, test_labels = data\n",
    "                  #test_data = test_data.cuda()\n",
    "                  y_pred = model(test_data)\n",
    "                  y_pred = torch.sigmoid(y_pred)\n",
    "                  test_pred.append(y_pred.cpu().detach().numpy())\n",
    "                  test_true.append(test_labels.numpy())\n",
    "            \n",
    "              test_true = np.concatenate(test_true)\n",
    "              test_pred = np.concatenate(test_pred)\n",
    "              val_auc_mean = np.mean(auc_roc_score(test_true, test_pred)) \n",
    "              model.train()\n",
    "\n",
    "              if best_val_auc < val_auc_mean:\n",
    "                 best_val_auc = val_auc_mean\n",
    "                 torch.save(model.state_dict(), 'aucm_pretrained_model.pth')\n",
    "\n",
    "              print ('Epoch=%s, BatchID=%s, Val_AUC=%.4f, Best_Val_AUC=%.4f'%(epoch, idx, val_auc_mean, best_val_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tFKAu80J1vzD"
   },
   "source": [
    "# **Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F-TTAuL51u4e"
   },
   "outputs": [],
   "source": [
    "# show auc roc scores for each task \n",
    "auc_roc_score(test_true, test_pred)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "07_Optimizing_Multi_Label_AUROC_Loss_with_DenseNet121_on_CheXpert.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
