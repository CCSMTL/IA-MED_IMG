

@Article{albumentations,
    AUTHOR = {Buslaev, Alexander and Iglovikov, Vladimir I. and Khvedchenya, Eugene and Parinov, Alex and Druzhinin, Mikhail and Kalinin, Alexandr A.},
    TITLE = {Albumentations: Fast and Flexible Image Augmentations},
    JOURNAL = {Information},
    VOLUME = {11},
    YEAR = {2020},
    NUMBER = {2},
    ARTICLE-NUMBER = {125},
    URL = {https://www.mdpi.com/2078-2489/11/2/125},
    ISSN = {2078-2489},
    DOI = {10.3390/info11020125}
}

@misc{densenet,
  doi = {10.48550/ARXIV.1608.06993},

  url = {https://arxiv.org/abs/1608.06993},

  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Densely Connected Convolutional Networks},

  publisher = {arXiv},

  year = {2016},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{convnext,
  doi = {10.48550/ARXIV.2201.03545},

  url = {https://arxiv.org/abs/2201.03545},

  author = {Liu, Zhuang and Mao, Hanzi and Wu, Chao-Yuan and Feichtenhofer, Christoph and Darrell, Trevor and Xie, Saining},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {A ConvNet for the 2020s},

  publisher = {arXiv},

  year = {2022},

  copyright = {Creative Commons Attribution 4.0 International}
}

@article{efficientnet,
  doi = {10.48550/ARXIV.1905.11946},

  url = {https://arxiv.org/abs/1905.11946},

  author = {Tan, Mingxing and Le, Quoc V.},

  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks},

  publisher = {arXiv},

  year = {2019},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{deit,
  doi = {10.48550/ARXIV.2012.12877},

  url = {https://arxiv.org/abs/2012.12877},

  author = {Touvron, Hugo and Cord, Matthieu and Douze, Matthijs and Massa, Francisco and Sablayrolles, Alexandre and Jégou, Hervé},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Training data-efficient image transformers &amp; distillation through attention},

  publisher = {arXiv},

  year = {2020},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{image16x16,
  doi = {10.48550/ARXIV.2010.11929},

  url = {https://arxiv.org/abs/2010.11929},

  author = {Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},

  publisher = {arXiv},

  year = {2020},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{chexnet,
  doi = {10.48550/ARXIV.1711.05225},

  url = {https://arxiv.org/abs/1711.05225},

  author = {Rajpurkar, Pranav and Irvin, Jeremy and Zhu, Kaylie and Yang, Brandon and Mehta, Hershel and Duan, Tony and Ding, Daisy and Bagul, Aarti and Langlotz, Curtis and Shpanskaya, Katie and Lungren, Matthew P. and Ng, Andrew Y.},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {CheXNet: Radiologist-Level Pneumonia Detection on Chest X-Rays with Deep Learning},

  publisher = {arXiv},

  year = {2017},

  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{chexpert,
  doi = {10.48550/ARXIV.1901.07031},

  url = {https://arxiv.org/abs/1901.07031},

  author = {Irvin, Jeremy and Rajpurkar, Pranav and Ko, Michael and Yu, Yifan and Ciurea-Ilcus, Silviana and Chute, Chris and Marklund, Henrik and Haghgoo, Behzad and Ball, Robyn and Shpanskaya, Katie and Seekins, Jayne and Mong, David A. and Halabi, Safwan S. and Sandberg, Jesse K. and Jones, Ricky and Larson, David B. and Langlotz, Curtis P. and Patel, Bhavik N. and Lungren, Matthew P. and Ng, Andrew Y.},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Image and Video Processing (eess.IV), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering},

  title = {CheXpert: A Large Chest Radiograph Dataset with Uncertainty Labels and Expert Comparison},

  publisher = {arXiv},

  year = {2019},

  copyright = {Creative Commons Attribution 4.0 International}
}

@software{yolo,
author = {Jocher, Glenn},
doi = {10.5281/zenodo.3908559},
license = {GPL-3.0},
month = {5},
title = {{YOLOv5 by Ultralytics}},
url = {https://github.com/ultralytics/yolov5},
version = {7.0},
year = {2020}
}


@misc{jfhealthcare,
    title={Weakly Supervised Lesion Localization With Probabilistic-CAM Pooling},
    author={Wenwu Ye and Jin Yao and Hui Xue and Yi Li},
    year={2020},
    eprint={2005.14480},
    archivePrefix={arXiv},
    primaryClass={cs.CV}
}

@misc{hierarchical,
  doi = {10.48550/ARXIV.1911.06475},

  url = {https://arxiv.org/abs/1911.06475},

  author = {Pham, Hieu H. and Le, Tung T. and Tran, Dat Q. and Ngo, Dat T. and Nguyen, Ha Q.},

  keywords = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Interpreting chest X-rays via CNNs that exploit hierarchical disease dependencies and uncertainty labels},

  publisher = {arXiv},

  year = {2019},

  copyright = {Creative Commons Attribution 4.0 International}
}


@article{DeepAUC,
  doi = {10.48550/ARXIV.2012.03173},

  url = {https://arxiv.org/abs/2012.03173},

  author = {Yuan, Zhuoning and Yan, Yan and Sonka, Milan and Yang, Tianbao},

  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Optimization and Control (math.OC), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},

  title = {Large-scale Robust Deep AUC Maximization: A New Surrogate Loss and Empirical Studies on Medical Image Classification},

  publisher = {arXiv},

  year = {2020},

  copyright = {arXiv.org perpetual, non-exclusive license}
}
@misc{adamw,
  doi = {10.48550/ARXIV.1711.05101},

  url = {https://arxiv.org/abs/1711.05101},

  author = {Loshchilov, Ilya and Hutter, Frank},

  keywords = {Machine Learning (cs.LG), Neural and Evolutionary Computing (cs.NE), Optimization and Control (math.OC), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},

  title = {Decoupled Weight Decay Regularization},

  publisher = {arXiv},

  year = {2017},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{timm,
  author = {Ross Wightman},
  title = {PyTorch Image Models},
  year = {2019},
  publisher = {GitHub},
  journal = {GitHub repository},
  doi = {10.5281/zenodo.4414861},
  howpublished = {\url{https://github.com/rwightman/pytorch-image-models}}
}

@article{chexzero,
author = {Tiu, Ekin and Talius, Ellie and Patel, Pujan and Langlotz, Curtis and Ng, Andrew and Rajpurkar, Pranav},
year = {2022},
month = {09},
pages = {1-8},
title = {Expert-level detection of pathologies from unannotated chest X-ray images via self-supervised learning},
volume = {6},
journal = {Nature Biomedical Engineering},
doi = {10.1038/s41551-022-00936-9}
}


@InProceedings{ dash,
  author    = { {S}hammamah {H}ossain },
  title     = { {V}isualization of {B}ioinformatics {D}ata with {D}ash {B}io },
  booktitle = { {P}roceedings of the 18th {P}ython in {S}cience {C}onference },
  pages     = { 126 - 133 },
  year      = { 2019 },
  editor    = { {C}hris {C}alloway and {D}avid {L}ippa and {D}illon {N}iederhut and {D}avid {S}hupe },
  doi       = { 10.25080/Majora-7ddc1dd1-012 }
}

@misc{onecyclelr,
  doi = {10.48550/ARXIV.1708.07120},

  url = {https://arxiv.org/abs/1708.07120},

  author = {Smith, Leslie N. and Topin, Nicholay},

  keywords = {Machine Learning (cs.LG), Computer Vision and Pattern Recognition (cs.CV), Neural and Evolutionary Computing (cs.NE), Machine Learning (stat.ML), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Super-Convergence: Very Fast Training of Neural Networks Using Large Learning Rates},

  publisher = {arXiv},

  year = {2017},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{chexnet_critic,
  author = {Lauren Oakden-Rayner},
  title = {CheXNet: an in-depth review},
  url = {https://laurenoakdenrayner.com/2018/01/24/chexnet-an-in-depth-review/},
  year = {2018}

}

@misc{chexpert_critic,
  author = {Lauren Oakden-Rayner},
  title = {Half a million x-rays! First impressions of the Stanford and MIT chest x-ray datasets},
  url = {https://laurenoakdenrayner.com/2019/02/25/half-a-million-x-rays-first-impressions-of-the-stanford-and-mit-chest-x-ray-datasets/},
  year = {2019}

}

@misc{metrics,
  doi = {10.48550/ARXIV.2201.09044},

  url = {https://arxiv.org/abs/2201.09044},

  author = {Gösgens, Martijn and Zhiyanov, Anton and Tikhonov, Alexey and Prokhorenkova, Liudmila},

  keywords = {Machine Learning (cs.LG), Discrete Mathematics (cs.DM), Probability (math.PR), FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Mathematics, FOS: Mathematics},

  title = {Good Classification Measures and How to Find Them},

  publisher = {arXiv},

  year = {2022},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@misc{clahe,
  doi = {10.48550/ARXIV.2006.13873},

  url = {https://arxiv.org/abs/2006.13873},

  author = {Siddhartha, Manu and Santra, Avik},

  keywords = {Image and Video Processing (eess.IV), Computer Vision and Pattern Recognition (cs.CV), Machine Learning (cs.LG), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences, I.4.9; I.4.3},

  title = {COVIDLite: A depth-wise separable deep neural network with white balance and CLAHE for detection of COVID-19},

  publisher = {arXiv},

  year = {2020},

  copyright = {arXiv.org perpetual, non-exclusive license}
}

@article {resolution,
	author = {Haque, Md Inzamam Ul and Dubey, Abhishek K. and Hinkle, Jacob D.},
	title = {The Effect of Image Resolution on Automated Classification of Chest X-rays},
	elocation-id = {2021.07.30.21261225},
	year = {2021},
	doi = {10.1101/2021.07.30.21261225},
	publisher = {Cold Spring Harbor Laboratory Press},
	abstract = {Deep learning models have received much attention lately for their ability to achieve expert-level performance on the accurate automated analysis of chest X-rays. Although publicly available chest X-ray datasets include high resolution images, most models are trained on reduced size images due to limitations on GPU memory and training time. As compute capability continues to advance, it will become feasible to train large convolutional neural networks on high-resolution images. To verify that this will lead to increased performance, we perform a systematic evaluation to measure the effect of input chest X-ray image resolution on accuracy. This study is based on the publicly available MIMIC-CXR-JPG dataset, comprising 377,110 high resolution chest X-ray images, and provided with 14 labels to the corresponding free-text radiology reports. Our original hypothesis that increased resolution would lead to higher accuracy held true for some but not all of the tasks. We find, interestingly, that tasks that require a large receptive field are better suited to downscaled input images, and we verify this qualitatively by inspecting effective receptive fields and class activation maps of trained models. Finally, we show that stacking an ensemble across resolutions outperforms each individual learner at all input resolutions while providing interpretable scale weights, suggesting that multi-scale features are crucially important to information extraction from high-resolution chest X-rays.Competing Interest StatementThe authors have declared no competing interest.Funding StatementThis manuscript has been authored in part by UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the US Department of Energy (DOE). The US government retains and the publisher, by accepting the article for publication, acknowledges that the US government retains a nonexclusive, paid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for US government purposes. DOE will provide public access to these results offederally sponsored research in accordance with the DOE Public Access Plan (http://energy.gov/downloads/doe-public-access-plan). This work has been supported in part by the Artificial Intelligence Initiative at Oak Ridge National Laboratory. This research used resources of the Compute and Data Environment for Science (CADES) at the Oak Ridge National Laboratory, which is supported by the Office of Science of DOE.Author DeclarationsI confirm all relevant ethical guidelines have been followed, and any necessary IRB and/or ethics committee approvals have been obtained.YesThe details of the IRB/oversight body that provided approval or exemption for the research described are given below:The present study used the MIMIC-CXR-JPG dataset which was made available after registration under the PhysioNet Credentialed Health Data Use Agreement 1.5.0 for the MIMIC-CXR-JPG - chest radiographs with structured labels (v2.0.0). The MIMIC-CXR and MIMIC-CXR-JPG datasets were acquired at Beth Israel Deaconess Medical Center (BIDMC, Boston, MA) under a project approved by the Institutional Review Board of BIDMC. Requirement for individual patient consent in the original data collection was waived because the project did not impact clinical care and all protected health information was removed.All necessary patient/participant consent has been obtained and the appropriate institutional forms have been archived.YesI understand that all clinical trials and any other prospective interventional studies must be registered with an ICMJE-approved registry, such as ClinicalTrials.gov. I confirm that any such study reported in the manuscript has been registered and the trial registration ID is provided (note: if posting a prospective study registered retrospectively, please provide a statement in the trial ID field explaining why the study was not registered in advance).YesI have followed all appropriate research reporting guidelines and uploaded the relevant EQUATOR Network research reporting checklist(s) and other pertinent material as supplementary files, if applicable.YesThis manuscript uses the MIMIC-CXR-JPG dataset, which is available to the public upon request. https://physionet.org/content/mimic-cxr-jpg/2.0.0/},
	URL = {https://www.medrxiv.org/content/early/2021/08/01/2021.07.30.21261225},
	eprint = {https://www.medrxiv.org/content/early/2021/08/01/2021.07.30.21261225.full.pdf},
	journal = {medRxiv}
}

@article{noisy_label_medical,
  author    = {Davood Karimi and
               Haoran Dou and
               Simon K. Warfield and
               Ali Gholipour},
  title     = {Deep learning with noisy labels: exploring techniques and remedies
               in medical image analysis},
  journal   = {CoRR},
  volume    = {abs/1912.02911},
  year      = {2019},
  url       = {http://arxiv.org/abs/1912.02911},
  eprinttype = {arXiv},
  eprint    = {1912.02911},
  timestamp = {Thu, 02 Jan 2020 18:08:18 +0100},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1912-02911.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{noisy_label_review,
  author    = {Hwanjun Song and
               Minseok Kim and
               Dongmin Park and
               Jae{-}Gil Lee},
  title     = {Learning from Noisy Labels with Deep Neural Networks: {A} Survey},
  journal   = {CoRR},
  volume    = {abs/2007.08199},
  year      = {2020},
  url       = {https://arxiv.org/abs/2007.08199},
  eprinttype = {arXiv},
  eprint    = {2007.08199},
  timestamp = {Wed, 22 Jul 2020 12:09:15 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-2007-08199.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@misc{polycam,
  doi = {10.48550/ARXIV.2204.13359},

  url = {https://arxiv.org/abs/2204.13359},

  author = {Englebert, Alexandre and Cornu, Olivier and De Vleeschouwer, Christophe},

  keywords = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences},

  title = {Poly-CAM: High resolution class activation map for convolutional neural networks},

  publisher = {arXiv},

  year = {2022},

  copyright = {Creative Commons Attribution 4.0 International}
}

@article{gradcam,
	doi = {10.1007/s11263-019-01228-7},

	url = {https://doi.org/10.1007%2Fs11263-019-01228-7},

	year = 2019,
	month = {oct},

	publisher = {Springer Science and Business Media {LLC}
},

	volume = {128},

	number = {2},

	pages = {336--359},

	author = {Ramprasaath R. Selvaraju and Michael Cogswell and Abhishek Das and Ramakrishna Vedantam and Devi Parikh and Dhruv Batra},

	title = {Grad-{CAM}: Visual Explanations from Deep Networks via Gradient-Based Localization},

	journal = {International Journal of Computer Vision}
}

@misc{cross-bias,
  doi = {10.48550/ARXIV.2002.02497},

  url = {https://arxiv.org/abs/2002.02497},

  author = {Cohen, Joseph Paul and Hashir, Mohammad and Brooks, Rupert and Bertrand, Hadrien},

  keywords = {Image and Video Processing (eess.IV), Machine Learning (cs.LG), Quantitative Methods (q-bio.QM), Machine Learning (stat.ML), FOS: Electrical engineering, electronic engineering, information engineering, FOS: Electrical engineering, electronic engineering, information engineering, FOS: Computer and information sciences, FOS: Computer and information sciences, FOS: Biological sciences, FOS: Biological sciences},

  title = {On the limits of cross-domain generalization in automated X-ray prediction},

  publisher = {arXiv},

  year = {2020},

  copyright = {arXiv.org perpetual, non-exclusive license}
}
